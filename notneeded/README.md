# Cowork Agent â€” A Reverse-Engineered Cowork-Style AI Agent

A fully functional recreation of Anthropic's **Cowork mode** agent architecture, built from the ground up by studying how Cowork works. This project demonstrates a deep understanding of Cowork's internals â€” from its XML-tagged system prompt design to its tool-calling loop, provider abstraction, and sandbox execution model.

Built entirely with a **free, self-hosted stack**: Ollama (local LLM) + SearXNG (metasearch engine). No paid API keys required.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cowork Agent                         â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   CLI    â”‚â”€â”€â–¶â”‚  Agent Loop  â”‚â”€â”€â–¶â”‚ Prompt Builder â”‚  â”‚
â”‚  â”‚Interface â”‚   â”‚  (15 iter)   â”‚   â”‚  (XML-tagged)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                        â”‚                                â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚           â–¼            â–¼            â–¼                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚    â”‚  Ollama    â”‚ â”‚ OpenAI  â”‚ â”‚Anthropic â”‚             â”‚
â”‚    â”‚  Provider  â”‚ â”‚Provider â”‚ â”‚ Provider â”‚             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚           â”‚                                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚    â”‚          Tool Registry              â”‚              â”‚
â”‚    â”‚  bash Â· read Â· write Â· edit Â· glob  â”‚              â”‚
â”‚    â”‚  grep Â· web_search Â· web_fetch      â”‚              â”‚
â”‚    â”‚  todo_write                         â”‚              â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                           â”‚
         â–¼                           â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Ollama     â”‚           â”‚   SearXNG    â”‚
  â”‚  (Local LLM) â”‚           â”‚ (Metasearch) â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Design Decisions (Matching Cowork)

**XML-Tagged System Prompt** â€” Just like Cowork, the system prompt is assembled at runtime with XML sections: `<env>`, `<tools>`, `<behavioral_rules>`, `<runtime_context>`. Date and time are injected dynamically on each prompt build (not static).

**Tool Schemas in Prompt** â€” For Ollama (which lacks native tool_use), tool schemas are embedded directly in the system prompt with JSON calling instructions. The agent parses `tool_calls` JSON blocks from free-form LLM output using regex + a 3-tier JSON sanitizer.

**Agent Loop with Recovery** â€” The core loop runs up to 15 iterations. It includes truncation detection (catches when Ollama hits `num_predict` limits mid-JSON), intent-without-action detection (catches when the LLM says "I'll create the file" but doesn't actually call a tool), and automatic retry with nudging â€” all with caps to prevent infinite loops.

**Provider Abstraction** â€” Swappable LLM backends: Ollama (JSON-in-prompt), OpenAI (native tool_use), Anthropic (native tool_use). All implement the same `BaseLLMProvider` interface via a factory pattern.

**Parallel Tool Execution** â€” Multiple tool calls in a single LLM response are executed concurrently using `asyncio.gather()`.

## Project Structure

```
cowork_agent/
â”œâ”€â”€ __init__.py              # Package init (v0.1.0)
â”œâ”€â”€ __main__.py              # python -m entry point
â”œâ”€â”€ main.py                  # CLI arg parsing, workspace resolution, tool registration
â”œâ”€â”€ setup.py                 # Package setup with optional extras
â”œâ”€â”€ requirements.txt         # Core: pyyaml, httpx
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ default_config.yaml  # Default settings (provider, model, timeouts, etc.)
â”‚   â””â”€â”€ settings.py          # Config loader with YAML merge + env var overrides
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ agent.py             # Agent loop â€” the orchestrator
â”‚   â”œâ”€â”€ models.py            # Data models (Message, ToolCall, ToolResult, AgentResponse, ToolSchema)
â”‚   â”œâ”€â”€ prompt_builder.py    # XML-tagged system prompt assembly
â”‚   â”œâ”€â”€ tool_registry.py     # Tool registry with parallel execution
â”‚   â””â”€â”€ providers/
â”‚       â”œâ”€â”€ base.py          # Abstract provider + ProviderFactory
â”‚       â”œâ”€â”€ ollama.py        # Ollama provider (JSON parsing, truncation detection)
â”‚       â”œâ”€â”€ openai_provider.py
â”‚       â””â”€â”€ anthropic_provider.py
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ bash.py              # Shell command execution with timeout
â”‚   â”œâ”€â”€ read.py              # File reading with line limits
â”‚   â”œâ”€â”€ write.py             # File writing with auto-mkdir
â”‚   â”œâ”€â”€ edit.py              # Exact string replacement (read-first guard)
â”‚   â”œâ”€â”€ glob_tool.py         # File pattern matching (pathlib.glob)
â”‚   â”œâ”€â”€ grep_tool.py         # Content search (ripgrep with Python fallback)
â”‚   â”œâ”€â”€ web_search.py        # Web search via claude_web_tools
â”‚   â”œâ”€â”€ web_fetch.py         # URL fetch + processing via claude_web_tools
â”‚   â””â”€â”€ todo.py              # In-memory task tracking
â”‚
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ behavioral_rules.py  # Agent personality and behavioral guidelines
â”‚
â”œâ”€â”€ interfaces/
â”‚   â””â”€â”€ cli.py               # Interactive terminal with spinner, ANSI colors, /commands
â”‚
â””â”€â”€ sandbox/
    â”œâ”€â”€ Containerfile         # Ubuntu 22.04 + Python 3.11 + ripgrep
    â”œâ”€â”€ compose.yml           # Podman/Docker compose (agent + ollama + searxng)
    â””â”€â”€ searxng-config/
        â””â”€â”€ settings.yml      # SearXNG config (google + bing + duckduckgo)

claude_web_tools/
â”œâ”€â”€ __init__.py              # WebSearch, WebFetch exports
â”œâ”€â”€ web_search.py            # SearXNG-backed search with result formatting
â”œâ”€â”€ web_fetch.py             # URL fetching with 4-tier HTMLâ†’Markdown conversion
â”œâ”€â”€ html_to_markdown.py      # trafilatura â†’ markdownify â†’ BS4 â†’ regex fallback
â”œâ”€â”€ llm_processor.py         # Ollama-based content summarization
â”œâ”€â”€ cache.py                 # TTL-based response cache
â”œâ”€â”€ config.py                # Configuration management
â”œâ”€â”€ models.py                # SearchResult, SearchResponse, FetchResult
â”œâ”€â”€ requirements.txt         # httpx, trafilatura, markdownify, beautifulsoup4
â””â”€â”€ searxng-config/
    â””â”€â”€ settings.yml
```

## Quick Start

### Option A: Run directly (recommended for development)

**Prerequisites:** Ollama and SearXNG running locally.

```bash
# 1. Start Ollama (if not already running)
ollama serve

# 2. Pull a model
ollama pull qwen2.5:7b    # or any model you prefer

# 3. Start SearXNG (using Podman/Docker)
podman run -d -p 8888:8080 --name searxng docker.io/searxng/searxng:latest

# 4. Install dependencies
cd cowork_agent
pip install -r requirements.txt
pip install -r ../claude_web_tools/requirements.txt

# 5. Run the agent
python -m cowork_agent -p ollama -m qwen2.5:7b -v
```

### Option B: Run with containers (full stack)

```bash
cd cowork_agent/sandbox

# Start all services (Ollama + SearXNG + Agent)
podman-compose up -d

# Pull a model into the Ollama container
podman exec -it cowork-ollama ollama pull qwen2.5:7b

# Attach to the agent
podman start -ai cowork-agent
```

### CLI Usage

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ğŸ¤– Cowork Agent v0.1.0          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  Workspace: /Users/you/Documents/project

You â–¸ search about latest AI news and create a blog in html
  â ‹ Thinking... (3s)
  ğŸŒ Executing web_search...
  âœ“ ### Search Results for: latest AI news ...
  â ™ Thinking... (12s)
  âœï¸ Executing write...
  âœ“ Successfully wrote 4889 bytes (113 lines) to blog.html
  â ¹ Thinking... (5s)

Agent â–¸ I've created a comprehensive blog post at blog.html with the latest AI news.

You â–¸ /help
You â–¸ /todos
You â–¸ /clear
You â–¸ /exit
```

### CLI Arguments

| Flag | Description | Default |
|------|-------------|---------|
| `-p`, `--provider` | LLM provider (`ollama`, `openai`, `anthropic`) | `ollama` |
| `-m`, `--model` | Model name | `qwen3-vl:235b-instruct-cloud` |
| `-v`, `--verbose` | Enable debug logging | `false` |
| `--workspace` | Working directory path | Interactive prompt |
| `-c`, `--config` | Custom config file path | `default_config.yaml` |

## How It Works (Matching Cowork's Flow)

```
User Input
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Build System     â”‚â—€â”€â”€ XML-tagged prompt with:
â”‚ Prompt           â”‚    â€¢ Date/time injection
â”‚                  â”‚    â€¢ Tool schemas
â”‚                  â”‚    â€¢ Behavioral rules
â”‚                  â”‚    â€¢ Todo context
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Call LLM         â”‚â”€â”€â”€â”€ Ollama: parse tool_calls from text
â”‚                  â”‚     OpenAI/Anthropic: native tool_use
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
 Tool      No tools
 Calls     â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Return text to user
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execute Tools    â”‚â”€â”€â”€â”€ Parallel via asyncio.gather()
â”‚ (in parallel)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   Add results to
   memory, loop â”€â”€â”€â”€â”€â”€â–¶ Back to "Build System Prompt"
```

## Recovery Mechanisms

The agent handles common failure modes of local LLMs:

| Issue | Detection | Recovery |
|-------|-----------|----------|
| **Truncated JSON** | Ollama `done_reason: "length"` + unclosed brackets | Re-prompt with "output shorter content" (max 2 retries) |
| **Intent without action** | LLM says "I'll create..." but no `tool_calls` JSON | Nudge: "output the tool_calls block now" (max 2 nudges) |
| **Invalid JSON** | Literal newlines in JSON strings | 3-tier sanitizer: direct â†’ character-walk escape â†’ brute force |
| **Bloated context** | Large tool outputs re-sent to LLM | Auto-truncation to 3K chars per message |

## Configuration

Edit `cowork_agent/config/default_config.yaml`:

```yaml
llm:
  provider: "ollama"
  model: "qwen2.5:7b"
  temperature: 0.7
  max_tokens: 16384

providers:
  ollama:
    base_url: "http://localhost:11434"
    timeout: 300

agent:
  max_iterations: 15
  workspace_dir: "./workspace"
```

## What This Project Demonstrates

This project is a proof-of-concept showing that Cowork's architecture can be understood, replicated, and extended:

- **System prompt engineering** â€” XML-tagged, dynamically assembled prompts with runtime context injection (date, todos, tool schemas)
- **Tool-calling on local LLMs** â€” Embedding tool schemas in prompts and parsing structured JSON from free-form text, with robust error recovery for the quirks of local models
- **Agent loop design** â€” Iterative tool-use loop with parallel execution, conversation memory, truncation handling, and graceful degradation
- **Provider abstraction** â€” Clean separation between the agent logic and the LLM backend, making it trivial to swap between Ollama, OpenAI, and Anthropic
- **Self-hosted web tools** â€” WebSearch (SearXNG) and WebFetch (4-tier HTMLâ†’Markdown) that mirror Cowork's web capabilities without any paid APIs
- **Container-based sandbox** â€” Podman/Docker compose setup mirroring Cowork's isolated execution environment

## Tech Stack

| Component | Technology |
|-----------|-----------|
| LLM Inference | Ollama (local), OpenAI, Anthropic |
| Web Search | SearXNG (self-hosted metasearch) |
| HTML Processing | trafilatura + markdownify + BeautifulSoup4 |
| Content Summarization | Ollama (local LLM) |
| HTTP Client | httpx (async) |
| Configuration | PyYAML |
| Containerization | Podman / Docker |
| Search Backend | ripgrep (with Python re fallback) |

## License

This project was created for educational and demonstration purposes â€” to showcase an understanding of AI agent architecture patterns as implemented in Anthropic's Cowork mode.

---

*Built by studying Cowork, for the Cowork team.*
